# -*- coding: utf-8 -*-
"""Customer_Segmentation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gSQpPfCCHTVYljr-cXJH8w2hPHYez79G

PRML MINOR PROJECT: Project4 ( Dataset: Retail )

    [ A company that sells some of the product, and you want to know how well the selling
    performance of the product. You have the data that we can analyze, but what kind of analysis can
    we do? Well, we can segment customers based on their buying behavior on the market. Your task
    is to classify the data into the possible types of customers which the retailer can encounter. ]

    Gaurav Naval B21EE020
    Garvit Gangwal B21EE019
    Abhaymani Singh B21EE001
"""

import warnings
warnings.filterwarnings('ignore')

!pip install chart-studio

"""**Importing necessary** **Libraries**"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import datetime as dt

import sklearn
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from scipy.cluster.hierarchy import linkage
from scipy.cluster.hierarchy import dendrogram
from scipy.cluster.hierarchy import cut_tree
import plotly.graph_objs as go
from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.cluster.hierarchy import fcluster
from sklearn.preprocessing import LabelEncoder
import plotly.offline as pyoff
from sklearn.decomposition import PCA

"""# Data Preprocessing"""

# Importing The Dataset

Data = pd.read_excel('/content/Online Retail.xlsx')
Data.to_csv('Online Retail.csv', index=False)

Data

# Calculating the missing values

Data.isnull().sum()

# Calculating the Missing Values % contribution in DF

df_null = round(100*(Data.isnull().sum())/len(Data), 2)
df_null

# Droping rows having missing values

Data = Data.dropna()
Data.shape

# data info

Data.info()

# data description

Data.describe()

# Converting InvoiceDate to datetime and adding new columns of 'Date' , 'Month' and 'Year'

Data['InvoiceDate'] = pd.to_datetime(Data['InvoiceDate'])
Data['Date'] = Data['InvoiceDate'].dt.date
Data['Month'] = Data['InvoiceDate'].dt.month
Data['Year'] = Data['InvoiceDate'].dt.year
Data

# remove the stockcode column
Data = Data.drop(columns=['StockCode'])

# instantiate the label encoder
le = LabelEncoder()

# fit and transform the 'Country' column
Data['Country'] = le.fit_transform(Data['Country'])

# Removing duplicates and negative values from Data

Data.drop_duplicates(inplace=True)
Data = Data[(Data['Quantity'] > 0) & (Data['UnitPrice'] > 0)]
Data

#creating YearMonth field for the ease of reporting and visualization
Data['InvoiceYearMonth'] = Data['InvoiceDate'].map(lambda date: 100*date.year + date.month)

#calculate Revenue for each row and create a new dataframe with YearMonth - Revenue columns
Data['Revenue'] = Data['UnitPrice'] * Data['Quantity']

revenue = Data.groupby(['InvoiceYearMonth'])['Revenue'].sum().reset_index()
revenue['InvoiceYearMonth'] = revenue['InvoiceYearMonth'].astype(str).apply(lambda x: x[:4] + '-' + x[4:])
revenue

"""# Data Visualisation"""

# create scatter plot of monthly revenue
sns.set_style('whitegrid')
plt.figure(figsize=(12,6))
sns.scatterplot(x='InvoiceYearMonth', y='Revenue', data=revenue, color='blue', s=50)
sns.lineplot(x='InvoiceYearMonth', y='Revenue', data=revenue)
plt.title('Monthly Revenue of Shop')
plt.xlabel('Year-Month')
plt.ylabel('Revenue ($)')
plt.show()

# calculate monthly revenue and growth rate
revenue['MonthlyGrowth'] = revenue['Revenue'].pct_change()

# filter out the first month
revenue = revenue[revenue['InvoiceYearMonth'] != 201012]

# create scatter plot of monthly growth rate with hover effect
sns.set_style('whitegrid')
plt.figure(figsize=(12,6))
scatterplot = sns.scatterplot(x='InvoiceYearMonth', y='MonthlyGrowth', data=revenue, color='blue', s=50)
sns.lineplot(x='InvoiceYearMonth', y='MonthlyGrowth', data=revenue)
plt.title('Monthly Growth Rate')
plt.xlabel('Year-Month')
plt.ylabel('Growth Rate')
plt.show()

# Monthly revenue analysis
monthly_revenue = Data.groupby(pd.Grouper(key='InvoiceDate', freq='M')).sum()
plt.figure(figsize=(10,6))
sns.lineplot(x=monthly_revenue.index, y='Revenue', data=monthly_revenue)
sns.scatterplot(x=monthly_revenue.index, y='Revenue', data=monthly_revenue, color='blue', s=50)
plt.title('Monthly Revenue')
plt.xlabel('Month')
plt.ylabel('Revenue')
plt.show()

# Top 10 products by sales
top_products = Data.groupby('Description').sum().sort_values(by='Revenue', ascending=False).head(10)
plt.figure(figsize=(10,6))
sns.barplot(x='Revenue', y=top_products.index, data=top_products)
plt.title('Top 10 Products by Sales')
plt.xlabel('Revenue')
plt.ylabel('Product Description')
plt.show()

# Top 10 customers by sales
top_customers = Data.groupby('CustomerID').sum().sort_values(by='Revenue', ascending=False).head(10)
plt.figure(figsize=(10,6))
sns.barplot(x=top_customers.index,y='Revenue', data=top_customers, palette='Blues_d', dodge=0.5)
plt.title('Top 10 Customers by Sales')
plt.ylabel('Sales Amount')
plt.xlabel('Customer ID')
plt.show()

# Creating count plot for 'Country'

sns.countplot(x='Country', data=Data)
plt.xticks(rotation=90)

# Creating count plot for 'Month'

sns.countplot(x='Month', data=Data)
plt.title('Sales by Month')

# Creating count plot for 'Year'

sns.countplot(x='Year', data=Data)
plt.title('Sales by Year')

# Group the data by date and calculate the sum of sales
sales_by_date = Data.groupby('Date')['Revenue'].sum().reset_index()

# Create the countplot
plt.figure(figsize=(15,7))
sns.lineplot(x='Date', y='Revenue', data=sales_by_date)
plt.title('Sales by Date')
plt.xticks(rotation=45)
plt.show()

"""# K-Means Clustering"""

# Clustering on the basis of RFM values

# Calculate RFM values for each customer
recency = Data.groupby('CustomerID')['InvoiceDate'].max().apply(lambda x: (pd.to_datetime('2011-12-10') - x).days)
frequency = Data.groupby('CustomerID').nunique()['InvoiceNo']
monetary = Data.groupby('CustomerID').sum()['Revenue']

rfm = pd.concat([recency, frequency, monetary], axis=1)
rfm.columns = ['Recency', 'Frequency', 'Monetary']

# Standardize the data
scaler = StandardScaler()
rfm_scaled = scaler.fit_transform(rfm)
rfm_scaled
rfm_df = pd.DataFrame(rfm_scaled,columns = ['Recency', 'Frequency', 'Monetary'])

# Determine the optimal number of clusters using elbow method
wcss_new = []
for i in range(1, 11):
    kmeans_new = KMeans(n_clusters=i, init='k-means++', random_state=0)
    kmeans_new.fit(rfm_scaled)
    wcss_new.append(kmeans_new.inertia_)

plt.plot(range(1, 11), wcss_new)
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

# Determine the optimal number of clusters using Silhouette score
curr_new = 0
n_new=0
silhouette_scores_new=[]
for i in range(2, 11):
    kmeans0 = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans0.fit(rfm_scaled)
    score_new = silhouette_score(rfm_scaled, kmeans0.labels_)
    silhouette_scores_new.append(score_new)
    if score_new>curr_new:
        curr_new = score_new
        n_new=i

print("Silhouette score for optimal clusters is ",curr_new)
print("Optimal number of clusters are ",n_new)

# Train the KMeans model
kmeans_rfm = KMeans(n_clusters=n_new, init='k-means++', random_state=0)
kmeans_rfm.fit(rfm_scaled)

# Add cluster labels to the dataset
rfm_df['Cluster'] = kmeans_rfm.labels_

# Plot silhouette scores
plt.plot(range(2, 11), silhouette_scores_new)
plt.title('Silhouette Analysis')
plt.xlabel('Number of clusters')
plt.ylabel('Silhouette score')
plt.show()

# Visualize the clusters
sns.scatterplot(x='Recency', y='Frequency', hue='Cluster', data=rfm_df)
plt.title('RFM Clusters')
plt.xlabel('Recency')
plt.ylabel('Frequency')
plt.show()

sns.scatterplot(x='Recency', y='Monetary', hue='Cluster', data=rfm_df)
plt.title('RFM Clusters')
plt.xlabel('Recency')
plt.ylabel('Monetary')
plt.show()

sns.scatterplot(x='Frequency', y='Monetary', hue='Cluster', data=rfm_df)
plt.title('RFM Clusters')
plt.ylabel('Monetary')
plt.xlabel('Frequency')
plt.show()

#Clustering on the basis of quantity

# Extracting the relevant columns
data = Data[['CustomerID', 'Quantity']]

# Grouping data by customer ID and calculating the total quantity purchased
customer_data = data.groupby(['CustomerID']).agg({'Quantity': 'sum'})

# Creating an array of total quantity purchased
X = np.array(customer_data['Quantity'])

# Reshaping the array
X = X.reshape(-1, 1)

# Using the Elbow method to determine the optimal number of clusters
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)
plt.plot(range(1, 11), wcss)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

# Determine the optimal number of clusters using Silhouette score
silhouette_scores = []
curr = 0
n=0
for i in range(2, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(X)
    score = silhouette_score(X, kmeans.labels_)
    silhouette_scores.append(score)
    if score>curr:
        curr = score
        n=i

print("Silhouette score for optimal clusters is ",curr)
print("Best number of clusters are ",n)

# Creating the K-means classifier with the optimal number of clusters
kmeans = KMeans(n_clusters=n, init='k-means++', random_state=42, max_iter=300)

# Fitting the data to the classifier
kmeans.fit(X)

# Adding the cluster labels to the customer data
customer_data['Cluster'] = kmeans.labels_

# Plot silhouette scores
plt.plot(range(2, 11), silhouette_scores)
plt.title('Silhouette Analysis')
plt.xlabel('Number of clusters')
plt.ylabel('Silhouette score')
plt.show()

# Visualising the clusters
sns.scatterplot(x='CustomerID', y='Quantity', hue='Cluster', data=customer_data)
plt.title('Customer Segments')
plt.xlabel('CustomerID')
plt.ylabel('Quantity')
plt.show()

# Extract necessary features
customer_data1 = Data.groupby(['CustomerID', 'Country']).agg({'InvoiceNo': 'nunique', 'Revenue': 'sum'}).reset_index()

# Standardize the data
scaler = StandardScaler()
customer_data_scaled = scaler.fit_transform(customer_data1)

# Determine the optimal number of clusters using elbow method
wcss4 = []
for i in range(1, 11):
    kmeans4 = KMeans(n_clusters=i, init='k-means++', random_state=0)
    kmeans4.fit(customer_data_scaled)
    wcss4.append(kmeans4.inertia_)

plt.plot(range(1, 11), wcss4)
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

# Determine the optimal number of clusters using Silhouette score
silhouette_scores11 = []
curr11 = 0
n11=0
for i in range(2, 11):
    kmeans11 = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans11.fit(X)
    score11 = silhouette_score(X, kmeans11.labels_)
    silhouette_scores11.append(score11)
    if score11>curr11:
        curr11 = score11
        n11=i

print("Silhouette score for optimal clusters is ",curr11)
print("Best number of clusters are ",n11)

# Plot silhouette scores
plt.plot(range(2, 11), silhouette_scores11)
plt.title('Silhouette Analysis')
plt.xlabel('Number of clusters')
plt.ylabel('Silhouette score')
plt.show()

# Train the KMeans model
kmeans_country = KMeans(n_clusters=n11, init='k-means++', random_state=0)
kmeans_country.fit(customer_data_scaled)

# Add cluster labels to the dataset
customer_data1['Cluster'] = kmeans_country.labels_

# Visualize the clusters
sns.scatterplot(x='Revenue', y='InvoiceNo', hue='Cluster', data=customer_data1, palette='viridis')
plt.title('Customer Clusters by Revenue and Invoice Count')
plt.xlabel('Revenue')
plt.ylabel('Invoice Count')

plt.show()

"""# Hierarchial Clustering"""

# Hierarchial Clustering on the basis of RFM

# Compute the linkage matrix
Z = linkage(rfm_scaled, method='ward')

# Plot the dendrogram
fig = plt.figure(figsize=(25, 10))
dn = dendrogram(Z)

# Use fcluster to obtain cluster labels
from scipy.cluster.hierarchy import fcluster
max_d = 20
cluster_labels = fcluster(Z, max_d, criterion='distance')

# Add cluster labels to the dataset
rfm_df['Cluster'] = cluster_labels

# Hierarchial Clustering on the basis of RFM

# Visualize the clusters
sns.scatterplot(x='Recency', y='Frequency', hue='Cluster', data=rfm_df)
plt.title('RFM Clusters')
plt.xlabel('Recency')
plt.ylabel('Frequency')
plt.show()

sns.scatterplot(x='Recency', y='Monetary', hue='Cluster', data=rfm_df)
plt.title('RFM Clusters')
plt.xlabel('Recency')
plt.ylabel('Monetary')
plt.show()

sns.scatterplot(x='Frequency', y='Monetary', hue='Cluster', data=rfm_df)
plt.title('RFM Clusters')
plt.ylabel('Monetary')
plt.xlabel('Frequency')
plt.show()

# Hierarchial Clustering on the basis of 'Quantity'

# Compute the linkage matrix
Z = linkage(X, method='ward')

# Plot the dendrogram
fig = plt.figure(figsize=(25, 10))
dn = dendrogram(Z)

# Hierarchial Clustering on the basis of 'Quantity'

# Use fcluster to obtain cluster labels
max_d = 20
cluster_labels = fcluster(Z, max_d, criterion='distance')

# Add cluster labels to the dataset
customer_data['Cluster'] = cluster_labels

# Visualize the clusters
sns.scatterplot(x='CustomerID', y='Quantity', hue='Cluster', data=customer_data)
plt.title('Customer Segments')
plt.xlabel('CustomerID')
plt.ylabel('Quantity')
plt.show()

# Hierarchial Clustering on the basis of 'Country'

# Compute the linkage matrix
Z = linkage(customer_data_scaled, method='ward')

# Plot the dendrogram
fig = plt.figure(figsize=(25, 10))
dn = dendrogram(Z)

# Hierarchial Clustering on the basis of 'Country'

# Use fcluster to obtain cluster labels
max_d = 20
cluster_labels = fcluster(Z, max_d, criterion='distance')

# Add cluster labels to the dataset
customer_data1['Cluster'] = cluster_labels

# Visualize the clusters
sns.scatterplot(x='Country', y='Revenue', hue='Cluster', data=customer_data1,edgecolor='k')
plt.title('Customer Segments')
plt.xlabel('Country')
plt.ylabel('Revenue')
plt.show()

"""# Applying PCA"""

# Scale the data
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
relevant_features = ['Quantity', 'CustomerID', 'Revenue']

# Perform PCA
pca = PCA(n_components=2)
pca.fit(Data[relevant_features])
pca_data = pca.transform(Data[relevant_features])
df_pca = pd.DataFrame(data=pca_data, columns=["PC1", "PC2"])

# Plot the results
fig, ax = plt.subplots(figsize=(10, 8)) # Set the figsize to 10x8
ax.scatter(pca_data[:,0], pca_data[:,1],s=30,edgecolor='k')
ax.set_xlabel('PC1')
ax.set_ylabel('PC2')
plt.show()

# Perform clustering on the reduced dimensional data
wcss_pca = []
for i in range(1, 11):
    kmeans_pca = KMeans(n_clusters=i, init='k-means++', random_state=0)
    kmeans_pca.fit(pca_data)
    wcss_pca.append(kmeans_pca.inertia_)

plt.plot(range(1, 11), wcss_pca)
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

print("Best number of clusters are ",3)

kmeans_pca1 = KMeans(n_clusters=3, init='k-means++', random_state=42)
kmeans_pca1.fit(pca_data)
labels = kmeans_pca1.labels_

plt.scatter(pca_data[:, 0], pca_data[:, 1], c=labels, cmap='viridis',edgecolor='k',s=20)
centers = kmeans_pca1.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5,edgecolor='k');
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.show()

"""# Applying LDA"""

from sklearn.cluster import KMeans
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# Select relevant features for LDA
lda_data = Data[['Quantity', 'UnitPrice', 'CustomerID']]

# Perform LDA
lda = LinearDiscriminantAnalysis(n_components=2)
X_lda = lda.fit_transform(lda_data, Data['CustomerID'])

# Apply KMeans clustering on the transformed data
kmeans_lda = KMeans(n_clusters=3, init='k-means++', random_state=42)
kmeans_lda.fit(X_lda)

# Visualize the clusters
plt.scatter(X_lda[:, 0], X_lda[:, 1], c=kmeans_lda.labels_, cmap='viridis',edgecolor='k')
plt.title('Clusters after LDA')
plt.xlabel('LD1')
plt.ylabel('LD2')
plt.show()